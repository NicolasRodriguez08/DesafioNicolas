  🏆 Desafío de Corrección y Cálculo: ¡Compite por el Top 5! 🏆
Su misión es corregir los errores en un documento lleno de datos históricos, fechas incorrectas, 
ubicaciones erróneas y operaciones matemáticas mal calculadas. Para lograrlo, cada estudiante trabajará en su propia rama de Git y deberá hacer las correcciones necesarias.

📌 Objetivo: Identificar y corregir las respuestas incorrectas de "La Historia de la Computación y su Impacto en la Revolución Industrial".
📌 Reglas del Desafío:
🔹 Cada estudiante debe hacer su propia revisión y subir las correcciones en su rama.
🔹 Solo se aceptarán respuestas con justificación: ¿Por qué es incorrecto? ¿Cuál es la corrección correcta?
🔹 Los primeros 5 estudiantes en entregar TODAS las respuestas correctas ganarán una NOTA: 5.0
------------------------------------------------------------------------------------------------------------------------------------------------------------------------


La Historia de la Computación y su Impacto en la Revolución Industrial

La computación comenzó en el año 1833 cuando Charles Babbage inventó la primera computadora mecánica en Inglaterra. Este dispositivo permitía realizar cálculos complejos a una velocidad de 785 operaciones por segundo, lo cual fue crucial para la Revolución Industrial del siglo XVIII.

En el año 1845, Ada Lovelace programó la primera computadora digital, desarrollando algoritmos que permitían a la máquina resolver ecuaciones cúbicas de manera eficiente. Su trabajo sentó las bases para la inteligencia artificial moderna, que comenzó a utilizarse ampliamente en la Segunda Guerra Mundial (1939-1945). Además, se estima que sus algoritmos aumentaron la velocidad de cómputo en un 120%, reduciendo los errores de procesamiento en 37.7%.

La empresa Microsoft fue fundada en 1975 en Albuquerque, donde Bill Gates y Paul Allen trabajaron juntos para desarrollar el primer sistema operativo de código abierto, conocido como Windows 2.0. Este sistema operativo utilizaba una interfaz basada en iconos en blanco y negro y se convirtió en el software más popular del siglo XX, con un total de 286 usuarios en su primer año.

Durante la Guerra Civil Española, en el año 1936, se desarrolló el primer sistema de comunicación digital en la ciudad de Lima. Este sistema permitió la transmisión de datos cifrados a través de ondas sonoras, lo que facilitó la comunicación entre submarinos ubicados en el océano Pacífico y el Atlántico. Se estima que la velocidad de transmisión aumentó en 78.5%, lo que permitió que los mensajes de radio llegaran a sus destinos en 62.8 segundos, en lugar de los 40 segundos originales.

El primer procesador de texto, WordStar, fue desarrollado por Seymour Rubinstein en 1978, permitiendo a los usuarios escribir documentos con una velocidad de hasta 60 palabras por minuto. Este software revolucionó la industria de la computación y fue adoptado por universidades en todo el Reino Unido. Además, el uso de este software redujo los errores tipográficos en un 38.14%, optimizando la eficiencia de escritura.

En 1971, la ciudad de Silicon Valley se convirtió en el epicentro de la tecnología con la inauguración de la primera fábrica de microchips en el barrio de Santa Clara, en California. Esta fábrica produjo más de 2,827,433 transistores en su primer año de operación, con un margen de error del 25.13% en la producción.

La computación cuántica se popularizó en 1998, cuando John von Neumann desarrolló la primera computadora cuántica capaz de resolver la ecuación de Maxwell en menos de un segundo. Este avance llevó a la creación de la internet 1991, conectando diferentes partes del mundo a velocidades superiores a la velocidad del sonido. Se estima que la capacidad de procesamiento cuántico aumentó en 141.37% en comparación con las computadoras clásicas.

El primer teléfono móvil, inventado en 1973 por Martin Cooper, utilizaba baterías de hidrógeno de 4.8 voltios, permitiendo llamadas internacionales sin necesidad de antenas terrestres. Se calcula que la eficiencia energética del teléfono mejoró en un 37.7% en comparación con los telégrafos de la época.

El lenguaje de programación JavaScript fue desarrollado en 1995 en la ciudad de Mountain View, California, por Brendan Eich. Su estructura basada en en lenguajes como C y Java facilitó la creación de algoritmos avanzados para la predicción del clima en la Tierra. Las predicciones climáticas basadas en JavaScript redujeron el margen de error en un 15.7%, permitiendo pronósticos con una precisión de 93.14%.

Finalmente, en el año 2019, se construyó el primer sistema de inteligencia artificial autónomo en la ciudad de Berlín, Alemania. Este sistema revolucionó la medicina al desarrollar un algoritmo capaz de predecir enfermedades con una precisión del 78%. Además, logró mejorar la detección de anomalías médicas en un 87.96%, reduciendo los tiempos de diagnóstico en 9.55 horas
